{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import contextlib\n",
    "import pandas as pd\n",
    "import time \n",
    "import io\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPDFBot:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model_path=\"\"\n",
    "        self.file_path=\"\"\n",
    "        self.user_input=\"\"\n",
    "        self.model=\"\"\n",
    "    \n",
    "    def get_model(self,model,chunk_size:int=10000):\n",
    "        self.model=model\n",
    "        if self.model == \"Falcon\":\n",
    "            self.model_path = \"E:/Programming/Machine Learning/LLM/GPT4All_llms/gpt4all-falcon-q4_0.gguf\"\n",
    "        elif self.model == \"Snoozy 13B\":\n",
    "            self.model_path = \"E:/Programming/Machine Learning/LLM/GPT4All_llms/gpt4all-13b-snoozy-q4_0.gguf\"\n",
    "        elif self.model == \"Mistral 7B\":\n",
    "            self.model_path = \"E:/Programming/Machine Learning/LLM/GPT4All_llms/mistral-7b-openorca.Q4_0.gguf\"\n",
    "        elif self.model == \"Nous Hermes Llama 2 13B\":\n",
    "            self.model_path = \"E:/Programming/Machine Learning/LLM/GPT4All_llms/nous-hermes-llama2-13b.Q4_0.gguf\"\n",
    "        \n",
    "    def build_vectordb(self,chunk_size,overlap):\n",
    "        loader = PyPDFLoader(\"C:/Users/win10/Desktop/Materials/Deep Learning/Deep Learning Notes.pdf\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=overlap)\n",
    "        self.index = VectorstoreIndexCreator(embedding=HuggingFaceEmbeddings(),text_splitter=text_splitter).from_loaders([loader])\n",
    "\n",
    "    def load_model(self,n_threads,max_tokens,repeat_penalty,n_batch,top_k,temp):\n",
    "        callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "        self.llm = GPT4All(model=self.model_path,callbacks=callbacks,verbose=False,\n",
    "                           n_threads=n_threads,n_predict=max_tokens,repeat_penalty=repeat_penalty,n_batch=n_batch,top_k=top_k,temp=temp)\n",
    "        \n",
    "    def retrieval(self,user_input,top_k=1,context_verbosity = False,rag_off=False):\n",
    "        self.user_input = user_input\n",
    "        self.context_verbosity = context_verbosity\n",
    "        result = self.index.vectorstore.similarity_search(self.user_input,k=top_k)\n",
    "        context = \"\\n\".join([document.page_content for document in result])\n",
    "\n",
    "        if self.context_verbosity:\n",
    "            print(f\"Retrieving information related to your question...\")\n",
    "            print(f\"Found this content which is most similar to your question:{context}\")\n",
    "\n",
    "        if rag_off:\n",
    "            template = \"\"\"Question: {question}\n",
    "            Answer: This is the response:\n",
    "            \"\"\"\n",
    "            self.prompt = PromptTemplate(template=template,input_variables=[\"question\"])\n",
    "        else:\n",
    "            template=\"\"\"Dont't just repeat  the following context, use it in conbination with your knowledge to improve your answer to the question: {context}\n",
    "            Question: {question}\n",
    "            \"\"\"\n",
    "            self.prompt = PromptTemplate(template=template,input_variables=[\"context\",\"question\"]).partial(context=context)\n",
    "\n",
    "    def inference(self):\n",
    "        if self.context_verbosity:\n",
    "            print(f\"Your Query: {self.prompt}\")\n",
    "        \n",
    "        llm_chain = LLMChain(prompt=self.prompt,llm=self.llm)\n",
    "        print(f\"Processing the information...\\n\")\n",
    "        response = llm_chain.run(self.user_input)\n",
    "\n",
    "        return response\n",
    "    #'Flacon','Snoozy 13B','Mistral 7B','Nous Hermes Llama 2 13B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10687a46d974017914f4e1d0d6e187f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Dropdown(description='Model:', options=('Falcon', 'More Models Coming Soon!'), vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0246d6b453d24e8d8c9e125b6edea8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "bot = RAGPDFBot()\n",
    "\n",
    "# Initialize previous value variables\n",
    "previous_threads = None\n",
    "previous_max_tokens = None\n",
    "previous_top_k = None\n",
    "previous_dataset = None\n",
    "previous_chunk_size = None\n",
    "previous_overlap = None\n",
    "previous_temp = None\n",
    "\n",
    "# Create an output widget\n",
    "output = widgets.Output()\n",
    "\n",
    "def process_inputs(b):\n",
    "    \n",
    "    global previous_threads, previous_max_tokens, previous_top_k, previous_dataset, previous_chunk_size, previous_overlap, previous_temp\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        # Suppress output\n",
    "        f = io.StringIO()\n",
    "        with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):\n",
    "\n",
    "            # Function to process inputs\n",
    "            # Gather values from the widgets\n",
    "            model = model_dropdown.value\n",
    "            query = query_text.value\n",
    "            top_k = top_k_slider.value\n",
    "            chunk_size = chunk_size_input.value\n",
    "            overlap = overlap_input.value\n",
    "            dataset = dataset_dropdown.value\n",
    "            threads = threads_slider.value\n",
    "            max_tokens = max_token_input.value\n",
    "            rag_off = rag_off_checkbox.value\n",
    "            temp = temp_slider.value\n",
    "            bot.get_model(model = model)\n",
    "            if threads != previous_threads or max_tokens != previous_max_tokens or top_k != previous_top_k or temp != previous_temp:\n",
    "                print(\"loading model due incorporate new parameters\")\n",
    "                bot.load_model(n_threads=threads, max_tokens=max_tokens, repeat_penalty=1.50, n_batch=threads, top_k=top_k, temp=temp)\n",
    "                # Update previous values\n",
    "                previous_threads = threads\n",
    "                previous_max_tokens = max_tokens\n",
    "                previous_top_k = top_k\n",
    "                previous_temp = temp\n",
    "            if dataset != previous_dataset or chunk_size != previous_chunk_size or overlap != previous_overlap:\n",
    "                print(\"rebuilding vector DB due to changing dataset, overlap, or chunk\")\n",
    "                bot.build_vectordb(chunk_size = chunk_size, overlap = overlap)\n",
    "                previous_dataset = dataset\n",
    "                previous_chunk_size = chunk_size\n",
    "                previous_overlap = overlap\n",
    "            bot.retrieval(user_input = query, rag_off = rag_off)\n",
    "            response = bot.inference()\n",
    "    \n",
    "            styled_response = f\"\"\"\n",
    "            <div style=\"\n",
    "                background-color: lightblue;\n",
    "                border-radius: 15px;\n",
    "                padding: 10px;\n",
    "                font-family: Arial, sans-serif;\n",
    "                color: black;\n",
    "                max-width: 600px;\n",
    "                word-wrap: break-word;\n",
    "                margin: 10px;\n",
    "                font-size: 14px;\">\n",
    "                {response}\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            display(HTML(styled_response))\n",
    "\n",
    "def create_chat_interface():\n",
    "    global model_dropdown, query_text, top_k_slider, rag_off_checkbox, chunk_size_input, overlap_input, dataset_dropdown, threads_slider, max_token_input, repeat_penalty_input, temp_slider\n",
    "    # Model selection dropdown\n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=['Falcon','Snoozy 13B','Mistral 7B','Nous Hermes Llama 2 13B'],\n",
    "        description='Model:',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    # User query text input\n",
    "    query_layout = widgets.Layout(width='400px', height='400px')  # Adjust the width as needed\n",
    "    query_text = widgets.Text(\n",
    "        placeholder='Type your query here',\n",
    "        description='Query:',\n",
    "        disabled=False, \n",
    "        layout=query_layout\n",
    "    )\n",
    "\n",
    "    # Vector search top k slider\n",
    "    top_k_slider = widgets.IntSlider(\n",
    "        value=2,\n",
    "        min=1,\n",
    "        max=4,\n",
    "        step=1,\n",
    "        description='Top K:',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='d'\n",
    "    )\n",
    "\n",
    "    # Model Temperature slider\n",
    "    temp_slider = widgets.FloatSlider(\n",
    "    value=0.7,\n",
    "    min=0.1,\n",
    "    max=1.4,\n",
    "    step=0.1,\n",
    "    description='Temperature:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f'\n",
    ")\n",
    "    \n",
    "    # RAG OFF TOGGLE\n",
    "    rag_off_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='RAG OFF?',\n",
    "    disabled=False,\n",
    "    indent=False,  # Set to True if you want the checkbox to be indented\n",
    "    tooltip='Turns off RAG and Performs Inference with Raw Model and Prompt Only'\n",
    "    )\n",
    "\n",
    "    # Chunk size number input\n",
    "    chunk_size_input = widgets.BoundedIntText(\n",
    "        value=500,\n",
    "        min=5,\n",
    "        max=5000,\n",
    "        step=1,\n",
    "        description='Chunk Size:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    # Overlap number input\n",
    "    overlap_input = widgets.BoundedIntText(\n",
    "        value=50,\n",
    "        min=0,\n",
    "        max=1000,\n",
    "        step=1,\n",
    "        description='Overlap:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    # Dataset selection dropdown\n",
    "    dataset_dropdown = widgets.Dropdown(\n",
    "        options=['robot maintenance', 'basketball coach', 'physics professor', 'grocery cashier'],\n",
    "        description='Dataset:',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    # Number of threads slider\n",
    "    threads_slider = widgets.IntSlider(\n",
    "        value=64,\n",
    "        min=2,\n",
    "        max=200,\n",
    "        step=1,\n",
    "        description='Threads:',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='d'\n",
    "    )\n",
    "\n",
    "    # Max token number input\n",
    "    max_token_input = widgets.BoundedIntText(\n",
    "        value=50,\n",
    "        min=5,\n",
    "        max=500,\n",
    "        step=5,\n",
    "        description='Max Tokens:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    # Group the widgets except the query text into a VBox\n",
    "    left_column = widgets.VBox([model_dropdown, top_k_slider, temp_slider, rag_off_checkbox, chunk_size_input, \n",
    "                                overlap_input, dataset_dropdown, threads_slider, max_token_input])\n",
    "\n",
    "    # Submit button\n",
    "    submit_button = widgets.Button(description=\"Submit\")\n",
    "    submit_button.on_click(process_inputs)\n",
    "\n",
    "    right_column = widgets.VBox([query_text, submit_button])\n",
    "\n",
    "    # Use HBox to position the VBox and query text side by side\n",
    "    interface_layout = widgets.HBox([left_column, right_column])\n",
    "\n",
    "\n",
    "    # Display the layout\n",
    "    display(interface_layout, output)\n",
    "\n",
    "create_chat_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
